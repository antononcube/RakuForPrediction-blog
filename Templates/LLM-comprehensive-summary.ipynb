{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM comprehensive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anton Antonov   \n",
    "[MathematicaForPrediction at WordPress](https://mathematicaforprediction.wordpress.com)   \n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)   \n",
    "April-May 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Generated on {Date.today.Str}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this computational Markdown file we apply different LLM prompts in order to comprehensively (and effectively) summarize large texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown file is intended to serve as a template for the initial versions of comprehensive text analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown template has corresponding notebooks versions: \n",
    "(i) [Wolfram Language notebook](https://community.wolfram.com/groups/-/m/t/3448842), \n",
    "(ii) [Raku-Jupyter notebook](),\n",
    "(iii) [Python-Jupyter notebook]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** All remarks in italics are supposed to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use LLM::Functions;\n",
    "use LLM::Prompts;\n",
    "use Text::SubParsers;\n",
    "use ML::FindTextualAnswer;\n",
    "use Data::Importers;\n",
    "use Data::Translators;\n",
    "use WWW::MermaidInk;\n",
    "use WWW::YouTube;\n",
    "use Hash::Merge;\n",
    "use Graph;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM access configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $conf4o-mini = llm-configuration('ChatGPT', model => 'gpt-4o-mini', max-tokens => 8192, temperature => 0.5);\n",
    "my $conf41-mini = llm-configuration('ChatGPT', model => 'gpt-4.1-mini', max-tokens => 8192, temperature => 0.5);\n",
    "my $conf-gemini-flash = llm-configuration('Gemini', model => 'gemini-2.0-flash', max-tokens => 8192, temperature => 0.5);\n",
    "\n",
    "# Choose an LLM access configuration or specify your own\n",
    "sink my $conf = $conf4o-mini;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an output language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink my $lang = 'English';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Chose whether to analyze a text from a file or to analyze the transcript of a YouTube video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest text from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my $txtFocus = slurp('');\n",
    "#text-stats($txtFocus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the transcript of a YouTube video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $txtFocus = youtube-transcript(\"eIR_OjWWjtE\", format => 'text');\n",
    "text-stats($txtFocus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The text ingested above is the transcript of the video [\"Live CEOing Ep 886: Design Review of LLMGraph\"](https://www.youtube.com/watch?v=ewU83vHwN8Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The transcript of a YouTube video can be obtained in several ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Raku package [\"WWW::YouTube\"](https://raku.land/zef:antononcube/WWW::YouTube)\n",
    "- On macOS, download the audio track and use the program [hear](https://sveinbjorn.org/hear) \n",
    "- Use the Python package [“pytube”](https://pypi.org/project/pytube/) (or [“pytubefix”](https://pypi.org/project/pytubefix/)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "llm-synthesize([llm-prompt(\"Summarize\"), $txtFocus, llm-prompt('Translated')('English')], e => $conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and tabulate text topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% html\n",
    "my $tblThemes = llm-synthesize(llm-prompt(\"ThemeTableJSON\")($txtFocus, \"article\", 30), e => $conf, form => sub-parser('JSON'):drop);\n",
    "$tblThemes ==> data-translation(field-names=><theme content>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mind-map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my @edges = \n",
    "    |llm-synthesize([\n",
    "        \"Make a JSON array with the graph edges of a concise mind-map for the following text.\",\n",
    "        \"Each edge is a dictionary with keys 'from' and 'to'.\",\n",
    "        \"Make sure the graph is connected and represents a mind-map.\",\n",
    "        \"TEXT START\",\n",
    "        $txtFocus,\n",
    "        \"TEXT END\",\n",
    "        llm-prompt(\"NothingElse\")(\"JSON\")\n",
    "    ], \n",
    "    e=>$conf,\n",
    "    form => sub-parser('JSON'):drop\n",
    "    );\n",
    "\n",
    "deduce-type(@edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% html\n",
    "my $g = Graph.new(@edges);\n",
    "\n",
    "$g.dot(engine => 'neato', background => '#1F1F1F'):svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mermaid-JS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Mermaid-JS code of a corresponding mind-map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "\n",
    "my $mmdBigPicture = llm-synthesize([\n",
    "    \"Create a concise mermaid-js mind-map for the text.\",\n",
    "    \"TEXT START\",\n",
    "    $txtFocus,\n",
    "    \"TEXT END\",\n",
    "    llm-prompt(\"NothingElse\")(\"correct mermaid-js in $lang\")\n",
    "], e=>$conf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sophisticated feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give sophisticated feedback using different “idea hats”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "my $sophFeed = llm-synthesize(llm-prompt(\"SophisticatedFeedback\")($txtFocus, 'HTML', :!ideas), e => $conf);\n",
    "\n",
    "$sophFeed.subst( / ^^ '```html'/, :g).subst(/ ^^ '```'/, :g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get answers to specific questions (if any.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $questions = q:to/END/;\n",
    "What technology? What it is used for?\"\n",
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "my $ans = llm-synthesize([$questions, $txtFocus], e => $conf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Instead of using the simple workflow above, more structured question-answer response can be obtained \n",
    "using the function `find-textual-answer` of the package [\"ML::FindTextualAnswer\"](https://raku.land/zef:antononcube/ML::FindTextualAnswer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "find-textual-answer(\n",
    "        $txtFocus, \n",
    "        [\"Who is talking?\", \"Which technology is discussed?\", \"What product(s) are discussed?\", \"Which versions?\"], \n",
    "        finder => llm-evaluator($conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted wisdom or cynical insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Choose one of the prompts \n",
    "[“ExtractArticleWisdom”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/ExtractArticleWisdom/) or \n",
    "[“FindPropagandaMessage”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/FindPropagandaMessage/).\n",
    "(The latter tends to be more fun.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $prompt = True ?? llm-prompt(\"ExtractArticleWisdom\")() !! llm-prompt(\"FindPropagandaMessage\");\n",
    "text-stats($prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% markdown\n",
    "\n",
    "my $sumIdea = \n",
    "    llm-synthesize([\n",
    "        $prompt,\n",
    "        'TEXT START',\n",
    "        $txtFocus,\n",
    "        'TEXT END'\n",
    "     ], e => $conf);\n",
    "\n",
    "$sumIdea.subst(/ ^^ '#' /, '###', :g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
