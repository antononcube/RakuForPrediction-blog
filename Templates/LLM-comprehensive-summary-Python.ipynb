{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM comprehensive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anton Antonov   \n",
    "[MathematicaForPrediction at WordPress](https://mathematicaforprediction.wordpress.com)   \n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)   \n",
    "April-June 2025"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from datetime import date\n",
    "f\"Generated on {date.today()}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this computational Markdown file we apply different LLM prompts in order to comprehensively (and effectively) summarize large texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown file is intended to serve as a template for the initial versions of comprehensive text analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown template has corresponding notebooks versions: \n",
    "(i) [Wolfram Language notebook](https://community.wolfram.com/groups/-/m/t/3448842), \n",
    "(ii) [Raku-Jupyter notebook](),\n",
    "(iii) [Python-Jupyter notebook]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** All remarks in italics are supposed to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from LLMFunctionObjects import *\n",
    "from LLMPrompts import *\n",
    "\n",
    "from pytubefix import Playlist, YouTube\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM access configurations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conf4o_mini = llm_configuration('ChatGPT', model = 'gpt-4o-mini', max_tokens = 8192,  temperature = 0.5)\n",
    "conf41_mini = llm_configuration('ChatGPT', model = 'gpt-4.1-mini', max_tokens = 8192,  temperature = 0.5)\n",
    "\n",
    "#conf_gemini_flash = llm_configuration('Gemini', model = 'gemini-2.0-flash', max_tokens = 8192, temperature = 0.5)\n",
    "\n",
    "# Choose an LLM access configuration\n",
    "conf = conf4o_mini"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an output language:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lang = 'English'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transcript:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_transcript(video_url, languages = (\"en\", )):\n",
    "    try:\n",
    "        # Check if the input has \"http://\" prefix\n",
    "        if \"http://\" in video_url or \"https://\" in video_url:\n",
    "            # Extract the video ID from the URL\n",
    "            video_id = video_url.split('v=')[1]\n",
    "        else:\n",
    "            # Assume the input is already a video ID\n",
    "            video_id = video_url\n",
    "\n",
    "        # Retrieve the transcript for the video\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)\n",
    "\n",
    "        # Concatenate the text from each transcript segment\n",
    "        transcript_text = ' '.join([segment['text'] for segment in transcript])\n",
    "\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {str(e)}')\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text stats function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def text_stats(text):\n",
    "    return {\n",
    "        \"char_count\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"line_count\": len(text.splitlines())\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph from edges:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_graph(edges):\n",
    "    \"\"\"\n",
    "    Generates a directed graph from a list of dictionaries representing edges.\n",
    "\n",
    "    Args:\n",
    "        edges: A list of dictionaries, where each dictionary has keys \"from\" and \"to\".\n",
    "\n",
    "    Returns:\n",
    "        A networkx DiGraph object.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        graph.add_edge(edge['from'], edge['to'])\n",
    "    return graph"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Chose whether to analyze a text from a file or to analyze the transcript of a YouTube video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest text from a file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# fileName = \"\";\n",
    "# with open('file.txt', 'r') as file:\n",
    "#     txtFocus = file.read()\n",
    "# text_stats(txtFocus)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the transcript of a YouTube video:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "txtFocus = get_transcript(\"ewU83vHwN8Y\")\n",
    "text_stats(txtFocus)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The text ingested above is the transcript of the video [\"Live CEOing Ep 886: Design Review of LLMGraph\"](https://www.youtube.com/watch?v=ewU83vHwN8Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The transcript of a YouTube video can be obtained in several ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Python package [“pytube”](https://pypi.org/project/pytube/) (or [“pytubefix”](https://pypi.org/project/pytubefix/)) \n",
    "- On macOS, download the audio track and use the program [hear](https://sveinbjorn.org/hear) \n",
    "- Use the Raku package [\"WWW::YouTube\"](https://raku.land/zef:antononcube/WWW::YouTube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the text:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary = llm_synthesize([llm_prompt(\"Summarize\"), txtFocus, llm_prompt('Translated')('English')], e = conf)\n",
    "display(Markdown(summary))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and tabulate text topics:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tblThemes = llm_synthesize(llm_prompt(\"ThemeTableJSON\")(txtFocus, \"article\", 30), e = conf, form = sub_parser('JSON', drop=True))\n",
    "display(HTML(pandas.DataFrame(tblThemes).to_html(index=True)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mind-map"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "edges = llm_synthesize([\n",
    "        \"Make a JSON array with the graph edges of a concise mind-map for the following text.\",\n",
    "        \"Each edge is a dictionary with keys 'from' and 'to'.\",\n",
    "        \"Make sure the graph is connected and represents a mind-map.\",\n",
    "        \"TEXT START\",\n",
    "        txtFocus,\n",
    "        \"TEXT END\",\n",
    "        llm_prompt(\"NothingElse\")(\"JSON\")\n",
    "    ], \n",
    "    e=conf,\n",
    "    form = sub_parser('JSON', drop=True)\n",
    ")\n",
    "\n",
    "graph = create_graph(edges)\n",
    "\n",
    "nx.draw(graph, with_labels=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Sophisticated feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give sophisticated feedback using different “idea hats”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sophFeed = llm_synthesize(llm_prompt(\"SophisticatedFeedback\")(txtFocus, 'HTML'), e = conf)\n",
    "\n",
    "sophFeed = re.sub(r'^```html', '', sophFeed, flags=re.MULTILINE)\n",
    "sophFeed = re.sub(r'^```', '', sophFeed, flags=re.MULTILINE)\n",
    "\n",
    "display(Markdown(sophFeed))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get answers to specific questions (if any.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "questions = \"\"\"\n",
    "What technology? What it is used for?\"\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ans = llm_synthesize([questions, txtFocus], e = conf)\n",
    "display(Markdown(ans))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "questions2 = [\"Who is talking?\", \"Which technology is discussed?\", \"What product(s) are discussed?\", \"Which versions?\"]\n",
    "\n",
    "ans2 = llm_synthesize([\n",
    "    \"Give a question-answer dictionary for the questions:\", \n",
    "    \"\\n\".join(questions2),\n",
    "    \"Over the text:\",\n",
    "    txtFocus, \n",
    "    llm_prompt('JSON')\n",
    "    ], \n",
    "    e = conf, form = sub_parser('JSON', drop=True)\n",
    ")\n",
    "\n",
    "display(HTML(pandas.DataFrame(ans2).to_html(index=False)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted wisdom or cynical insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Choose one of the prompts \n",
    "[“ExtractArticleWisdom”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/ExtractArticleWisdom/) or \n",
    "[“FindPropagandaMessage”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/FindPropagandaMessage/).\n",
    "(The latter tends to be more fun.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = llm_prompt(\"ExtractArticleWisdom\")() if True else llm_prompt(\"FindPropagandaMessage\")\n",
    "text_stats(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sumIdea = llm_synthesize([\n",
    "        prompt,\n",
    "        'TEXT START',\n",
    "        txtFocus,\n",
    "        'TEXT END'\n",
    "     ], e = conf);\n",
    "\n",
    "sumIdea = re.sub(r'^^#', '###', sumIdea, flags=re.MULTILINE)\n",
    "\n",
    "display(Markdown(sumIdea))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JNB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
