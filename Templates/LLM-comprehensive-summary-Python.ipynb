{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM comprehensive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anton Antonov   \n",
    "[MathematicaForPrediction at WordPress](https://mathematicaforprediction.wordpress.com)   \n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)   \n",
    "April-June 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generated on 2025-06-03'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "f\"Generated on {date.today()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this computational Markdown file we apply different LLM prompts in order to comprehensively (and effectively) summarize large texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown file is intended to serve as a template for the initial versions of comprehensive text analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** This Markdown template has corresponding notebooks versions: \n",
    "(i) [Wolfram Language notebook](https://community.wolfram.com/groups/-/m/t/3448842), \n",
    "(ii) [Raku-Jupyter notebook](),\n",
    "(iii) [Python-Jupyter notebook]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** All remarks in italics are supposed to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMFunctionObjects import *\n",
    "from LLMPrompts import *\n",
    "\n",
    "from pytubefix import Playlist, YouTube\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM access configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf4o_mini = llm_configuration('ChatGPT', model = 'gpt-4o-mini', max_tokens = 8192,  temperature = 0.5)\n",
    "conf41_mini = llm_configuration('ChatGPT', model = 'gpt-4.1-mini', max_tokens = 8192,  temperature = 0.5)\n",
    "\n",
    "#conf_gemini_flash = llm_configuration('Gemini', model = 'gemini-2.0-flash', max_tokens = 8192, temperature = 0.5)\n",
    "\n",
    "# Choose an LLM access configuration\n",
    "conf = conf4o_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an output language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'English'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(video_url, languages = (\"en\", )):\n",
    "    try:\n",
    "        # Check if the input has \"http://\" prefix\n",
    "        if \"http://\" in video_url or \"https://\" in video_url:\n",
    "            # Extract the video ID from the URL\n",
    "            video_id = video_url.split('v=')[1]\n",
    "        else:\n",
    "            # Assume the input is already a video ID\n",
    "            video_id = video_url\n",
    "\n",
    "        # Retrieve the transcript for the video\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)\n",
    "\n",
    "        # Concatenate the text from each transcript segment\n",
    "        transcript_text = ' '.join([segment['text'] for segment in transcript])\n",
    "\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {str(e)}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text stats function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(text):\n",
    "    return {\n",
    "        \"char_count\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"line_count\": len(text.splitlines())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph from edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(edges):\n",
    "    \"\"\"\n",
    "    Generates a directed graph from a list of dictionaries representing edges.\n",
    "\n",
    "    Args:\n",
    "        edges: A list of dictionaries, where each dictionary has keys \"from\" and \"to\".\n",
    "\n",
    "    Returns:\n",
    "        A networkx DiGraph object.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        graph.add_edge(edge['from'], edge['to'])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Chose whether to analyze a text from a file or to analyze the transcript of a YouTube video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest text from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileName = \"\";\n",
    "# with open('file.txt', 'r') as file:\n",
    "#     txtFocus = file.read()\n",
    "# text_stats(txtFocus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the transcript of a YouTube video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 36700, 'word_count': 7088, 'line_count': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtFocus = get_transcript(\"ewU83vHwN8Y\")\n",
    "text_stats(txtFocus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The text ingested above is the transcript of the video [\"Live CEOing Ep 886: Design Review of LLMGraph\"](https://www.youtube.com/watch?v=ewU83vHwN8Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The transcript of a YouTube video can be obtained in several ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Python package [“pytube”](https://pypi.org/project/pytube/) (or [“pytubefix”](https://pypi.org/project/pytubefix/)) \n",
    "- On macOS, download the audio track and use the program [hear](https://sveinbjorn.org/hear) \n",
    "- Use the Raku package [\"WWW::YouTube\"](https://raku.land/zef:antononcube/WWW::YouTube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The discussion focuses on the design review for version 14.3 of an LLM graph, which aims to facilitate more complex workflows and asynchronous calls to LLM functions. The conversation covers various aspects of the graph, including its structure, the use of nodes, and how inputs and outputs are handled, as well as the introduction of features like conditional nodes and listable functions. Overall, the team emphasizes the need for simplicity and clarity in the design while considering future enhancements and compatibility with existing frameworks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = llm_synthesize([llm_prompt(\"Summarize\"), txtFocus, llm_prompt('Translated')('English')], e = conf)\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and tabulate text topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction to LLM Graph</td>\n",
       "      <td>Discussion of LLM graph for version 14.3, focusing on knitting together LLM function calls for complex workflows and asynchronous execution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Example and Syntax</td>\n",
       "      <td>Preference for example first; explanation of graph nodes and their asynchronous nature, along with the concept of LLM functions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graph Evaluation Consistency</td>\n",
       "      <td>Concerns about consistency with directed acyclic graphs; emphasis on simplicity with single input and output ports.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Powerful Updates</td>\n",
       "      <td>Updates on LM synthesize submit and their implications for more agentic workflows and asynchronous code organization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Project Management and Evaluation</td>\n",
       "      <td>Discussion on project management, graph evaluation progress, and the need for clarity on responsibilities within the team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Graph Evaluation and Asynchronous Code</td>\n",
       "      <td>Exploration of graph evaluation concepts and how they relate to asynchronous code organization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Complex Workflow Examples</td>\n",
       "      <td>Examples of more complex workflows using LLM graphs, including free-form thought chains and comparisons of evaluators.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Output Definitions</td>\n",
       "      <td>Clarification on how outputs are defined in LLM graphs, including handling multiple terminal nodes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Template and Function Definitions</td>\n",
       "      <td>Discussion on the distinction between templates and functions, emphasizing their roles in LLM graphs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Listability and Conditionals</td>\n",
       "      <td>Introduction of listability and conditionals in LLM graphs, discussing their potential and packaging.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Design Considerations</td>\n",
       "      <td>Considerations for naming conventions and symmetry in the design of LLM graph nodes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Final Thoughts and Next Steps</td>\n",
       "      <td>Concluding remarks on the LLM graph design, with plans for future discussions on semantic ranking.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tblThemes = llm_synthesize(llm_prompt(\"ThemeTableJSON\")(txtFocus, \"article\", 30), e = conf, form = sub_parser('JSON', drop=True))\n",
    "display(HTML(pandas.DataFrame(tblThemes).to_html(index=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mind-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = llm_synthesize([\n",
    "        \"Make a JSON array with the graph edges of a concise mind-map for the following text.\",\n",
    "        \"Each edge is a dictionary with keys 'from' and 'to'.\",\n",
    "        \"Make sure the graph is connected and represents a mind-map.\",\n",
    "        \"TEXT START\",\n",
    "        txtFocus,\n",
    "        \"TEXT END\",\n",
    "        llm_prompt(\"NothingElse\")(\"JSON\")\n",
    "    ], \n",
    "    e=conf,\n",
    "    form = sub_parser('JSON', drop=True)\n",
    ")\n",
    "\n",
    "graph = create_graph(edges)\n",
    "\n",
    "nx.draw(graph, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Sophisticated feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give sophisticated feedback using different “idea hats”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary of the Idea\n",
       "The idea revolves around developing a new feature called \"LLM Graph,\" which is designed to facilitate the orchestration of complex workflows involving multiple Large Language Model (LLM) function calls. This feature aims to allow asynchronous calls and improve the handling of dependencies between different nodes in a graph-like structure. The LLM Graph will enable users to create more sophisticated workflows by connecting various LLM calls, effectively functioning as a powerful orchestrator for LLM interactions.\n",
       "\n",
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Hat Name</th>\n",
       "    <th>Perspective</th>\n",
       "    <th>Feedback</th>\n",
       "    <th>New Ideas</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>White Hat</td>\n",
       "    <td>Information and Facts</td>\n",
       "    <td>The LLM Graph aims to enhance the current capabilities of asynchronous LLM function calls by allowing users to create complex workflows. It will utilize nodes that communicate with each other to manage dependencies effectively.</td>\n",
       "    <td>Conduct user testing to gather data on usability, Analyze performance metrics of LLM Graph vs. traditional methods, Research existing similar technologies for benchmarking.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Black Hat</td>\n",
       "    <td>Judgement and Caution</td>\n",
       "    <td>There are significant risks in implementing this new feature. Potential issues include increased complexity in user interactions, possible performance bottlenecks, and the challenge of ensuring that all nodes function correctly without conflicts.</td>\n",
       "    <td>Develop a fallback mechanism for failed calls, Implement rigorous testing protocols, Create comprehensive documentation to mitigate user confusion.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Gray Hat</td>\n",
       "    <td>Cynicism and Skepticism</td>\n",
       "    <td>This may just be another buzzword in tech. Users could perceive the LLM Graph as a convoluted solution to a problem that may not even exist. There's a risk of overengineering.</td>\n",
       "    <td>Consider simplifying the core functionality, Ensure clear communication about the actual benefits, Reassess if existing solutions can meet user needs without this new feature.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Red Hat</td>\n",
       "    <td>Feelings and Emotion</td>\n",
       "    <td>This new feature sounds exciting and could empower users with more control over their workflows. However, there may be anxiety about the learning curve and the potential for errors in complex setups.</td>\n",
       "    <td>Host workshops to demonstrate the feature, Create a community forum for sharing experiences, Develop intuitive onboarding tutorials.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Yellow Hat</td>\n",
       "    <td>Benefits and Creativity</td>\n",
       "    <td>The LLM Graph has the potential to revolutionize how users interact with LLMs by streamlining processes and enabling more creative applications. It could lead to enhanced productivity and innovation.</td>\n",
       "    <td>Explore partnerships with educational institutions for pilot programs, Develop case studies showcasing successful implementations, Encourage user-generated content to inspire creativity.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Green Hat</td>\n",
       "    <td>Profitability and Potential</td>\n",
       "    <td>This feature could open up new revenue streams through premium services or subscriptions. It could attract more users looking for advanced functionalities in LLM interactions.</td>\n",
       "    <td>Introduce tiered pricing models based on usage, Create add-on features for specific industries, Explore licensing opportunities with third-party developers.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Blue Hat</td>\n",
       "    <td>Usefulness and Opportunities</td>\n",
       "    <td>The LLM Graph could be highly useful for developers and businesses looking to optimize their LLM applications. It presents opportunities for integration with existing platforms and tools.</td>\n",
       "    <td>Identify integration points with popular development environments, Collaborate with industry leaders for feedback, Create an API for external developers to build on.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Purple Hat</td>\n",
       "    <td>Assessment and Direction</td>\n",
       "    <td>Overall, the LLM Graph presents both exciting opportunities and significant challenges. While it has the potential to enhance user experience and profitability, careful consideration of implementation risks and user needs is crucial.</td>\n",
       "    <td>Establish a clear roadmap for development phases, Prioritize user feedback in iterations, Create a cross-functional team for ongoing assessment and improvement.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sophFeed = llm_synthesize(llm_prompt(\"SophisticatedFeedback\")(txtFocus, 'HTML'), e = conf)\n",
    "\n",
    "sophFeed = re.sub(r'^```html', '', sophFeed, flags=re.MULTILINE)\n",
    "sophFeed = re.sub(r'^```', '', sophFeed, flags=re.MULTILINE)\n",
    "\n",
    "display(Markdown(sophFeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get answers to specific questions (if any.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = \"\"\"\n",
    "What technology? What it is used for?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The technology being discussed in the transcript is **LLM Graph**, which stands for **Large Language Model Graph**. This technology is designed to facilitate complex workflows involving calls to large language models (LLMs) by knitting together multiple LLM function calls in a graph structure. Here are some key points regarding its purpose and functionality:\n",
       "\n",
       "### Purpose of LLM Graph\n",
       "1. **Complex Workflows**: LLM Graph allows for the orchestration of multiple LLM function calls, enabling more intricate workflows that can involve multiple steps and dependencies.\n",
       "  \n",
       "2. **Asynchronous Calls**: It supports asynchronous execution of LLM calls, which means that multiple tasks can be processed simultaneously without waiting for each individual task to complete before starting the next one.\n",
       "\n",
       "3. **Node Structure**: Each function call is represented as a node in the graph. Nodes can communicate with each other, allowing for a flexible and dynamic workflow.\n",
       "\n",
       "4. **Input and Output Management**: The graph can take inputs in the form of associations, and it has mechanisms to handle outputs, including the ability to specify which nodes should produce outputs.\n",
       "\n",
       "5. **Conditional Execution**: The technology incorporates conditional logic, allowing certain nodes to be executed only if specific conditions are met, thus optimizing resource usage.\n",
       "\n",
       "6. **Integration with Other Technologies**: The LLM Graph is designed to work alongside other technologies such as net graphs and proof graphs, ensuring a cohesive experience when building and evaluating complex models.\n",
       "\n",
       "### Usage Scenarios\n",
       "- **Chaining LLM Functions**: Users can chain multiple LLM function calls together, where the output of one call can serve as the input for another.\n",
       "- **Parallel Processing**: By utilizing listable functions, multiple inputs can be processed simultaneously, improving efficiency.\n",
       "- **Dynamic Dependencies**: The graph can adapt to changing dependencies between nodes, allowing for more flexible and responsive workflows.\n",
       "\n",
       "### Conclusion\n",
       "Overall, LLM Graph represents a significant advancement in how developers can leverage large language models for various applications, enabling more efficient, flexible, and powerful workflows. It abstracts away much of the complexity associated with managing multiple LLM calls, making it easier for users to build sophisticated applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = llm_synthesize([questions, txtFocus], e = conf)\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions2 = [\"Who is talking?\", \"Which technology is discussed?\", \"What product(s) are discussed?\", \"Which versions?\"]\n",
    "\n",
    "ans2 = llm_synthesize([\n",
    "    \"Give a question-answer dictionary for the questions:\", \n",
    "    \"\\n\".join(questions2),\n",
    "    \"Over the text:\",\n",
    "    txtFocus, \n",
    "    llm_prompt('JSON')\n",
    "    ], \n",
    "    e = conf, form = sub_parser('JSON', drop=True)\n",
    ")\n",
    "\n",
    "display(HTML(pandas.DataFrame(ans2).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted wisdom or cynical insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Choose one of the prompts \n",
    "[“ExtractArticleWisdom”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/ExtractArticleWisdom/) or \n",
    "[“FindPropagandaMessage”](https://www.wolframcloud.com/obj/antononcube/DeployedResources/Prompt/FindPropagandaMessage/).\n",
    "(The latter tends to be more fun.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 2329, 'word_count': 382, 'line_count': 35}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = llm_prompt(\"ExtractArticleWisdom\")() if True else llm_prompt(\"FindPropagandaMessage\")\n",
    "text_stats(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### SUMMARY\n",
       "The discussion revolves around the design review of the LLM graph in version 14.3, focusing on enhancing LLM calls for complex workflows and asynchronous operations. Participants include developers working on the project, discussing technical aspects, examples, and future improvements.\n",
       "\n",
       "### IDEAS:\n",
       "- The LLM graph aims to orchestrate multiple LLM calls for complex workflows.\n",
       "- Asynchronous calls to LLMs allow for more efficient processing.\n",
       "- The design avoids complicated concepts like ports, keeping it simple with nodes that depend on each other.\n",
       "- Each node in the graph serves as its own output port.\n",
       "- The LLM graph can evaluate inputs in a blocking manner, providing flexibility in execution.\n",
       "- Templates can be wrapped in LLM functions for easier use.\n",
       "- The design allows for chaining of nodes, enabling more complex evaluations.\n",
       "- The graph can handle multiple terminal nodes, outputting results in the order they are defined.\n",
       "- A conditional node can block execution based on specified criteria.\n",
       "- The potential for parallel processing is being explored with listable templates.\n",
       "- The distinction between LLM functions and node functions is crucial for clarity in execution.\n",
       "- The need for a consistent naming scheme for functions and templates is emphasized.\n",
       "- Evaluation graphs can help visualize and manage asynchronous code organization.\n",
       "- The design is evolving to accommodate user feedback and enhance functionality.\n",
       "- The integration of test functions with dependencies is being considered for better control.\n",
       "- The discussion highlights the importance of user experience in designing interfaces for these functions.\n",
       "- The team is focused on ensuring backward compatibility while introducing new features.\n",
       "- Future updates may include improved methods for dependency tracking in graphs.\n",
       "- The need for clear documentation and user guidance is recognized to facilitate understanding.\n",
       "- The project management aspect is being addressed to ensure timely progress.\n",
       "\n",
       "### QUOTES:\n",
       "- \"This is like a step up and it's an orchestrator of LM calls.\"\n",
       "- \"You can always wrap a prompt or a template in LLM function.\"\n",
       "- \"The assumption is a node is its own output port.\"\n",
       "- \"You can have plan one and plan two and pass it to two different nodes.\"\n",
       "- \"This is a very powerful update on LM syntiz submit.\"\n",
       "- \"We need to figure out what's happening with that.\"\n",
       "- \"This is the causal graph that you can get from our current evaluator.\"\n",
       "- \"This has been growing a lot in the past month or so.\"\n",
       "- \"At some point, we want to kind of stop and have like we have a decent run.\"\n",
       "- \"You could do this with code but there are a lot of things that need to happen.\"\n",
       "- \"This is a first example of more complex workflow.\"\n",
       "- \"The point is LLM graph right now LLM graph has the feature that it sub value is an evaluation.\"\n",
       "- \"You can imagine the applications here.\"\n",
       "- \"We are using information for property extraction.\"\n",
       "- \"If you specify both, we give an error message.\"\n",
       "- \"Everything is a normal node. Even the inputs are normal nodes.\"\n",
       "\n",
       "### HABITS:\n",
       "- Regularly review design documents to ensure clarity and consistency.\n",
       "- Engage in collaborative discussions to refine project features.\n",
       "- Test new features in real-world scenarios to assess usability.\n",
       "- Document changes and updates for future reference.\n",
       "- Prioritize user feedback to enhance the design process.\n",
       "- Schedule regular project management meetings to track progress.\n",
       "- Utilize clear naming conventions to avoid confusion in code.\n",
       "- Encourage open communication among team members for brainstorming.\n",
       "- Explore asynchronous processing techniques to improve efficiency.\n",
       "- Continuously assess the need for backward compatibility in updates.\n",
       "\n",
       "### FACTS:\n",
       "- LLM functions can be wrapped in templates for easier integration.\n",
       "- The design avoids using ports to maintain simplicity.\n",
       "- Each node can have multiple terminal outputs based on the structure.\n",
       "- Asynchronous processing allows for more efficient evaluations.\n",
       "- The current evaluator processes nodes sequentially.\n",
       "- The project team is focused on enhancing user experience and functionality.\n",
       "- The graph evaluation design is evolving to accommodate user needs.\n",
       "- The discussion includes considerations for dependency tracking in graphs.\n",
       "- The team is exploring parallel processing capabilities for templates.\n",
       "- The need for clear documentation is emphasized for user guidance.\n",
       "\n",
       "### REFERENCES:\n",
       "- LLM graph design documents.\n",
       "- Previous discussions on graph evaluation concepts.\n",
       "- Examples of asynchronous code organization.\n",
       "- User feedback reports on LLM functionalities.\n",
       "- Documentation on the integration of templates and functions.\n",
       "\n",
       "### RECOMMENDATIONS:\n",
       "- Implement clear naming conventions for functions and templates.\n",
       "- Enhance documentation to guide users through new features.\n",
       "- Schedule regular design reviews to incorporate feedback.\n",
       "- Explore additional tools for visualizing graph structures.\n",
       "- Prioritize user experience in the development of new functionalities.\n",
       "- Consider user testing for complex workflows to identify pain points.\n",
       "- Keep communication open among team members to foster collaboration.\n",
       "- Evaluate the feasibility of parallel processing in future updates.\n",
       "- Regularly assess the impact of changes on existing functionalities.\n",
       "- Stay flexible in design to accommodate evolving user needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sumIdea = llm_synthesize([\n",
    "        prompt,\n",
    "        'TEXT START',\n",
    "        txtFocus,\n",
    "        'TEXT END'\n",
    "     ], e = conf);\n",
    "\n",
    "sumIdea = re.sub(r'^^#', '###', sumIdea, flags=re.MULTILINE)\n",
    "\n",
    "display(Markdown(sumIdea))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JNB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
